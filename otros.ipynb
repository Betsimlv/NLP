{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de prueba sin un Modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataSocialMedia.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = ['message','sentiment']\n",
    "df = df[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message      1\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "message      0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())\n",
    "df = df.dropna()\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return cleaned_text\n",
    "df['message'] = df['message'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['message'] = df['message'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "df['message'] = df['message'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_extra_spaces(text):\n",
    "    # Eliminar los espacios en blanco al principio y al final de la cadena\n",
    "    text = text.strip()\n",
    "    # Reemplazar múltiples espacios consecutivos con un solo espacio\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "df['message'] = df['message'].apply(remove_extra_spaces)\n",
    "df[\"message\"] = df[\"message\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hacer miss venezuela retirarse miss uni seal p...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>imagino dentro plan accin est tener sistema vo...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seleccionar mejor organizacion tomando cuentas...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hermosas</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ando enojada clase robo super descarado tena f...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message sentiment\n",
       "0  hacer miss venezuela retirarse miss uni seal p...  negativo\n",
       "1  imagino dentro plan accin est tener sistema vo...  negativo\n",
       "2  seleccionar mejor organizacion tomando cuentas...  negativo\n",
       "3                                           hermosas  positivo\n",
       "4  ando enojada clase robo super descarado tena f...  negativo"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 2 2]\n",
      "['negativo' 'negativo' 'negativo' ... 'neutro' 'positivo' 'positivo']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convertir las etiquetas de texto a valores numéricos\n",
    "y_encoded = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "class_mapping = {index: label for index, label in enumerate(label_encoder.classes_)}\n",
    "y_labels = label_encoder.inverse_transform(y_encoded)\n",
    "\n",
    "# Imprimir los valores numéricos y las etiquetas correspondientes\n",
    "print(y_encoded)  \n",
    "print(y_labels)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message sentiment  label\n",
      "0  hacer miss venezuela retirarse miss uni seal p...  negativo      0\n",
      "1  imagino dentro plan accin est tener sistema vo...  negativo      0\n",
      "2  seleccionar mejor organizacion tomando cuentas...  negativo      0\n",
      "3                                           hermosas  positivo      2\n",
      "4  ando enojada clase robo super descarado tena f...  negativo      0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convertir las etiquetas de texto a valores numéricos\n",
    "df['label'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# Mapear los valores numéricos a las etiquetas originales\n",
    "class_mapping = {index: label for index, label in enumerate(label_encoder.classes_)}\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Division de los datos y tokenizacion / pruena 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"message\"]\n",
    "Y = df[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:  [b'excelente dios bendiga'\n",
      " b'amanda dios bendiga hiciste excelente excelente orgullo verte tv tan hermosa tan segura pblico hacindote'\n",
      " b'chamaaaaa reina universaaaal hiciste trabajo impecable dejaste nombre venezuela altooo']\n",
      "\n",
      "labels:  [2 2 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Crear conjuntos de datos de TensorFlow a partir de los datos divididos\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 20\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "for example, label in train_dataset.take(1):\n",
    "  print('texts: ', example.numpy()[:3])\n",
    "  print()\n",
    "  print('labels: ', label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'reina', 'corona', 'universo', 'miss', 'amanda',\n",
       "       'ganadora', 'venezuela', 'gracias', 't', 'mejor', 'hermosa',\n",
       "       'dios', 'ms', 'amandadudamel', 'robaron', 'siempre', 'hiciste',\n",
       "       'bella'], dtype='<U16')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 38,  13,  24,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  6,  13,  24,  18,  38,  38,  42, 277,   1,  28,  12,  28, 174,\n",
       "        836,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  1,   2,   1,  18,  35,  46, 107,  63,   8,   1,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_example = encoder(example)[:3].numpy()\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  b'excelente dios bendiga'\n",
      "Round-trip:  excelente dios bendiga                      \n",
      "\n",
      "Original:  b'amanda dios bendiga hiciste excelente excelente orgullo verte tv tan hermosa tan segura pblico hacindote'\n",
      "Round-trip:  amanda dios bendiga hiciste excelente excelente orgullo verte [UNK] tan hermosa tan segura pblico [UNK]          \n",
      "\n",
      "Original:  b'chamaaaaa reina universaaaal hiciste trabajo impecable dejaste nombre venezuela altooo'\n",
      "Round-trip:  [UNK] reina [UNK] hiciste trabajo impecable dejaste nombre venezuela [UNK]               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "  print(\"Original: \", example[n].numpy())\n",
    "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "División de los datos y tokenizacion / prueba 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<00V>\")\n",
    "tokenizer.fit_on_texts(df[\"message\"])\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900 294   1   1  46 180   1   1   1   1   1 953   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0]\n",
      "(22373, 173)\n"
     ]
    }
   ],
   "source": [
    "sequences =  tokenizer.texts_to_sequences(df[\"message\"])\n",
    "\n",
    "padded = pad_sequences(sequences, padding=\"post\")\n",
    "\n",
    "print(padded[1])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index) + 1\n",
    "embedding_dim = 16\n",
    "max_length = 300\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 20000\n",
    "\n",
    "training_sentences = df[\"message\"][0:training_size]\n",
    "testing_sentences = df[\"message\"][training_size:]\n",
    "training_labels = df[\"label\"][0:training_size]\n",
    "testing_labels = df[\"label\"][training_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sentences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sentences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.6922 - loss: 0.7737 - val_accuracy: 0.7560 - val_loss: 0.5759\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8145 - loss: 0.4548 - val_accuracy: 0.7922 - val_loss: 0.4990\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8656 - loss: 0.3612 - val_accuracy: 0.8255 - val_loss: 0.4733\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8893 - loss: 0.3022 - val_accuracy: 0.8437 - val_loss: 0.4524\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8987 - loss: 0.2730 - val_accuracy: 0.8361 - val_loss: 0.4563\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9071 - loss: 0.2499 - val_accuracy: 0.8428 - val_loss: 0.4634\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9120 - loss: 0.2294 - val_accuracy: 0.8483 - val_loss: 0.4754\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9135 - loss: 0.2290 - val_accuracy: 0.8428 - val_loss: 0.4952\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9196 - loss: 0.2143 - val_accuracy: 0.8449 - val_loss: 0.5151\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9193 - loss: 0.2095 - val_accuracy: 0.8453 - val_loss: 0.5268\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(training_padded,\n",
    "                    training_labels,\n",
    "                    epochs=10,\n",
    "                    validation_data=(testing_padded, testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9017 - loss: 0.3011\n",
      "Loss: 0.5268239378929138\n",
      "Accuracy: 0.8453434705734253\n"
     ]
    }
   ],
   "source": [
    "# Obtener las métricas de evaluación en los datos de prueba\n",
    "loss, accuracy = model.evaluate(testing_padded, testing_labels)\n",
    "\n",
    "# Imprimir los resultados de la evaluación\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "Clase predicha: 2\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que tienes una nueva secuencia de texto para hacer la predicción\n",
    "new_text = [\" bella la miss venezuela\"]\n",
    "\n",
    "# Tokenizar el texto utilizando el mismo tokenizer utilizado durante el entrenamiento\n",
    "new_sequences = tokenizer.texts_to_sequences(new_text)\n",
    "\n",
    "# Asegurarse de que las secuencias tengan la misma longitud que las secuencias de entrenamiento\n",
    "new_padded = pad_sequences(new_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Hacer la predicción utilizando el modelo entrenado\n",
    "predictions = model.predict(new_padded)\n",
    "\n",
    "# Obtener la clase predicha (índice con la mayor probabilidad)\n",
    "predicted_class = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "\n",
    "# Imprimir la clase predicha\n",
    "print(\"Clase predicha:\", predicted_class)\n",
    "\n",
    "# 1 neutro, 2 positvo, 0 negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prueba 3 / modelo con menos epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.6977 - loss: 0.7832 - val_accuracy: 0.7526 - val_loss: 0.6150\n",
      "Epoch 2/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7907 - loss: 0.5005 - val_accuracy: 0.7758 - val_loss: 0.5232\n",
      "Epoch 3/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8367 - loss: 0.4107 - val_accuracy: 0.8184 - val_loss: 0.4905\n",
      "Epoch 4/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8707 - loss: 0.3546 - val_accuracy: 0.8314 - val_loss: 0.4758\n",
      "Epoch 5/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8859 - loss: 0.3150 - val_accuracy: 0.8306 - val_loss: 0.4762\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "model_1.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "history = model_1.fit(training_padded,\n",
    "                    training_labels,\n",
    "                    epochs=5,\n",
    "                    validation_data=(testing_padded, testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8757 - loss: 0.3452\n",
      "Loss: 0.47616755962371826\n",
      "Accuracy: 0.8305941820144653\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss, accuracy = model_1.evaluate(testing_padded, testing_labels)\n",
    "\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Clase predicha: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_text = [\"horrile la nueva misss que se vaya\"]\n",
    "\n",
    "new_sequences = tokenizer.texts_to_sequences(new_text)\n",
    "\n",
    "\n",
    "new_padded = pad_sequences(new_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "predictions = model_1.predict(new_padded)\n",
    "\n",
    "\n",
    "predicted_class = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "\n",
    "\n",
    "print(\"Clase predicha:\", predicted_class)\n",
    "\n",
    "# 1 neutro, 2 positvo, 0 negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prueba 4 / modelo con Learning rate callback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.5915 - loss: 0.9149 - val_accuracy: 0.7370 - val_loss: 0.6401 - learning_rate: 0.0010\n",
      "Epoch 2/4\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7683 - loss: 0.5744 - val_accuracy: 0.7779 - val_loss: 0.5364 - learning_rate: 0.0013\n",
      "Epoch 3/4\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8192 - loss: 0.4609 - val_accuracy: 0.8217 - val_loss: 0.4815 - learning_rate: 0.0016\n",
      "Epoch 4/4\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8499 - loss: 0.3933 - val_accuracy: 0.8268 - val_loss: 0.4805 - learning_rate: 0.0020\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "model_4.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Learning rate callback\n",
    "scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/10))\n",
    "# Entrenar el modelo\n",
    "\n",
    "history = model_4.fit(training_padded,\n",
    "                    training_labels,\n",
    "                    epochs=4,\n",
    "                    validation_data=(testing_padded, testing_labels), \n",
    "                    callbacks=[scheduler])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8740 - loss: 0.3501\n",
      "Loss: 0.4805208444595337\n",
      "Accuracy: 0.8268015384674072\n"
     ]
    }
   ],
   "source": [
    "# Obtener las métricas de evaluación en los datos de prueba\n",
    "loss, accuracy = model_4.evaluate(testing_padded, testing_labels)\n",
    "\n",
    "# Imprimir los resultados de la evaluación\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8729 - loss: 0.3527\n",
      "Loss: 0.4754085838794708\n",
      "Accuracy: 0.8263801336288452\n"
     ]
    }
   ],
   "source": [
    "# Obtener las métricas de evaluación en los datos de prueba\n",
    "loss, accuracy = model_4.evaluate(testing_padded, testing_labels)\n",
    "\n",
    "# Imprimir los resultados de la evaluación\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Clase predicha: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_text = [\"miss venezuela\"]\n",
    "\n",
    "\n",
    "new_sequences = tokenizer.texts_to_sequences(new_text)\n",
    "\n",
    "\n",
    "new_padded = pad_sequences(new_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "predictions = model_4.predict(new_padded)\n",
    "\n",
    "\n",
    "predicted_class = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "\n",
    "print(\"Clase predicha:\", predicted_class)\n",
    "\n",
    "# 1 neutro, 2 positvo, 0 negativo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
